{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.85.tar.gz (49.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/codespace/.local/lib/python3.10/site-packages (from llama-cpp-python) (2.0.0)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/codespace/.local/lib/python3.10/site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from huggingface-hub) (2.32.3)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->huggingface-hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->huggingface-hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->huggingface-hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->huggingface-hub) (2024.7.4)\n",
      "Downloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.85-cp310-cp310-linux_x86_64.whl size=2777173 sha256=7191ffac2b49e8661ea5d61c20361beb1b407735d8cdae4ec87970ca85cbb967\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/3f/e8/4e/29a754f9175ef52b6481cd75e3af4de38bf6dfa9c2972f75d4\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: tqdm, diskcache, llama-cpp-python, huggingface-hub\n",
      "Successfully installed diskcache-5.6.3 huggingface-hub-0.24.5 llama-cpp-python-0.2.85 tqdm-4.66.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets use llama.cpp for fine-tuning an LLM \n",
    "\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets use a Llama 2 model with 7 billions of parameters and use a quantized model\n",
    "\n",
    "repo = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
    "model_name = \"llama-2-7b-chat.Q5_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model = Llama.from_pretrained( repo_id=repo, filename=model_name,\n",
    "                               verbose=False, temperature=0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '  A RAG (Red, Amber, Green) rating system is a common tool used in project management to assess project status and progress. However, whether it is a good idea to use a RAG rating system to evaluate aspirants for a data science job depends on several factors. Here are some points to consider:\\nPros of using a RAG rating system for evaluating data science aspirants:\\n1. Quantifiable and easy to understand: A RAG rating system provides a clear and simple way to evaluate the status of a project or candidate. It helps to quantify the progress or performance of the aspirant and provides a common language for evaluators to communicate with each other.\\n2. Focuses on critical aspects: A RAG rating system can help to identify the critical aspects of a project or candidate that require attention. By focusing on these areas, the evaluation process can be more effective and efficient.\\n3. Encourages early intervention: A RAG rating system can help to identify potential issues early on, allowing for early intervention and corrective action. This can be particularly useful in a data science context, where problems can be difficult to identify and correct.\\n4. Provides a framework for feedback: A RAG rating system provides a clear framework for providing feedback to aspirants. This can help to identify areas of strength and weakness, and provide guidance on how to improve.\\nCons of using a RAG rating system for evaluating data science aspirants:\\n1. Lacks context: A RAG rating system may not take into account the specific context of the data science project or aspirant. For example, a project that is in a highly regulated industry may require a different approach than a project in a more relaxed environment.\\n2. Too simplistic: A RAG rating system may not capture the full complexity of a data science project or aspirant. It may not take into account factors such as the quality of the data, the accuracy of the analysis, or the impact of the project on the organization.\\n3. Can be subjective: The interpretation of a RAG rating can be subjective, depending on the evaluator. This can lead to inconsistent evaluations and potential biases.\\n4. May'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "# msg = ChatCompletionMessage()\n",
    "# msg[\"role\"] = \"system\"    \n",
    "# msg[\"content\"] = ( \"This is optional\" )\n",
    "# messages.append(msg)\n",
    "\n",
    "msg = ChatCompletionMessage()\n",
    "msg[\"role\"] = \"user\"\n",
    "msg[\"content\"] = ( \"In context of LLM, do you think that a RAG is a good idea to evaluate aspirants for a data science job?\" )\n",
    "messages.append(msg)\n",
    "\n",
    "result = model.create_chat_completion(messages)\n",
    "result[\"choices\"][0][\"message\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
